# FFMPEG MCP Server - AI-Assisted Music Video Creation

This project is an intelligent MCP server for creating professional music videos through AI-guided FFmpeg processing. It combines FastTrack analysis, Build Detective CI validation, and hierarchical subagent orchestration to deliver cost-effective, high-quality video processing.

## What This Project Does

**Primary Mission**: Transform video processing from manual, error-prone workflows into intelligent, automated systems that understand content, predict optimal strategies, and deliver professional results at scale.

**Core Value Proposition**: 
- **99.7% Cost Reduction**: $0.02 AI analysis vs $125+ manual decisions
- **95% Success Rate**: AI-guided processing vs 70% manual reliability  
- **Professional Quality**: Automated crossfades, beat-sync, format optimization
- **Intelligent Orchestration**: Multi-agent coordination for complex workflows

## How We Approach Video Processing

### Music Video Creation Strategy

**Philosophy**: Separate video and audio processing streams for maximum flexibility and performance.

- **Video Processing**: Focus on visual effects, timing, and transitions (drop audio with `-an`)
- **Audio Integration**: External high-quality audio sources (MP3, WAV) replace video audio
- **Smart Assembly**: AI-guided combination of processed video + prepared audio
- **Performance Optimization**: Simpler processing without unnecessary audio stream overhead

## ‚ö†Ô∏è MANDATORY CONSULTATION RULES ‚ö†Ô∏è

**CRITICAL**: Always consult before architectural changes to prevent over-engineering cycles.

**NEVER** change base images, package managers, or core dependencies without explicit permission:
- **Tech Stack Changes**: Alpine‚ÜíDebian, pip‚ÜíUV, Python versions require approval
- **Always Present Options**: "Fix Alpine deps vs switch to Debian - which?" 
- **Wait for Permission**: No implementation until explicit user approval
- **VIOLATION = IMMEDIATE STOP**

**Root Cause Analysis Protocol**:
1. Compare CI environment vs local environment EXACTLY
2. Check if main branch has this issue  
3. Look for SIMPLEST explanation first (often 3-line fixes)
4. If fix requires >10 lines, PAUSE and reconsider approach

## Project Organization Standards

### File Organization Strategy

**Zero Root Directory Pollution**: AI-generated files NEVER appear in project root.

**Directory Structure**:
```
/tmp/kompo/haiku-ffmpeg/     # All generated content
‚îú‚îÄ‚îÄ generated-videos/        # Final outputs
‚îú‚îÄ‚îÄ youtube-downloads/       # Source material
‚îú‚îÄ‚îÄ test-files/             # Experiments
‚îî‚îÄ‚îÄ temp/                   # Processing cache
```

**File Value Classification**:
- **Keep**: Source code (`src/`), configurations, documentation, small tests (<10MB)
- **Archive**: Historical analysis, old experiments, legacy configs  
- **Temp**: AI reports with timestamps, runtime artifacts, large files (>10MB)

**Git Rules**: Never stage timestamped reports, logs, build artifacts, or large binaries

## AI Intelligence Framework

**The system combines two specialized AI agents for cost-effective, high-quality video processing:**

### FastTrack Subagent (Video Analysis)
- **Ultra-Low Cost**: $0.02-0.05 per analysis (99.7% cost savings vs manual)
- **Frame Alignment Detection**: Automatically identifies and fixes timing issues
- **Smart Strategy Selection**: AI chooses optimal FFmpeg approach based on content
- **Quality Assurance**: PyMediaInfo verification with confidence scoring  
- **Creative Transitions**: 44 FFmpeg xfade effects with intelligent selection
- **Fallback Mode**: Works offline without API keys for development

### Build Detective Subagent (CI Analysis)  
- **Pre-Commit Validation**: Local CI testing prevents GitHub Actions failures
- **Maven Multi-Module Expertise**: Comprehensive build log parsing
- **Token-Saving Protocol**: BD analysis before expensive LLM operations
- **Confidence Framework**: High confidence (>8/10) = direct implementation

**Delegation Strategy**: Automatically route tasks to specialized subagents based on content analysis.

## Multi-Agent Orchestration

**YOLO operates as master orchestrator** coordinating specialized subagents:

- **FastTrack**: Video analysis and strategy selection
- **Komposteur**: Beat-synchronization and S3 infrastructure  
- **VideoRenderer**: FFmpeg optimization and crossfade processing
- **VDVIL**: DJ-mixing and audio composition
- **Build Detective**: CI/build failure analysis

**Coordination Benefits**:
- **Parallel Processing**: Multiple agents work simultaneously
- **Seamless Handoffs**: FastTrack analysis ‚Üí Audio (VDVIL) ‚Üí Beat-sync (Komposteur) ‚Üí Crossfades (VideoRenderer) ‚Üí Final assembly (YOLO)
- **Quality Assurance**: Master oversight with consistent standards
- **Cost Optimization**: Intelligent resource sharing with budget awareness

## Lessons Learned

**Architectural Anti-Pattern**: Git submodules for source dependencies create tight coupling, repository bloat, and maintenance nightmares. Use API-first integration with Maven dependencies instead.

**LLM Over-Engineering Prevention**: 
- **"3-Line Rule"**: If root cause likely <10 lines, analyze more than implement
- **Environment Comparison**: Compare CI vs local before architectural changes  
- **Symptom vs Root Cause**: Avoid "whack-a-mole" fixing cycles
- **Progressive Error Resolution**: Each fix should produce NEW, more specific errors

## Technical Implementation Details

### Development vs Production Strategy
**Local Development**: Use latest JARs from `~/.m2/repository/` for fast iteration without network delays
**Production CI**: GitHub Packages with authentication for reproducible, secure builds
**Auto-Detection**: System automatically prefers local development JARs when available

### Video Format Strategy  
**Final User Output**: Always use YUV420P format for maximum compatibility (VLC, QuickTime)
**Intermediate Processing**: YUV444P acceptable for internal workflows (higher quality)
**Verification**: Test final videos in standard players before delivery

### Local CI Validation
**Command**: `uv run python test_basic_ci.py && echo "‚úÖ CI PASSED"`
**Components**: Basic validation, Build Detective analysis, video format strategy testing
**Git Hook Integration**: Available but not enabled - user observes performance first

## Project Navigation

**Clean Repository Structure** (Root contains only essential files):
- **Core**: `src/`, `tests/`, `docs/`, `tools/`, `examples/`
- **Configuration**: `pyproject.toml`, `CLAUDE.md`, `Dockerfile`, `Makefile`
- **Generated Content**: `/tmp/kompo/haiku-ffmpeg/` (never in project root)
- **Archives**: `archive/` for historical analysis, configs, media files

**Quick Navigation by Domain**:
```bash
# Video Processing Intelligence
src/haiku_subagent.py              # FastTrack core system
tools/ft                           # FastTrack CLI
docs/FASTTRACK_*.md               # Documentation

# CI/Build Analysis  
tools/scripts/bd_*.py              # Build Detective scripts
docs/ai-agents/BUILD_DETECTIVE_*.md # BD documentation

# Music Video Creation
integration/komposteur/            # Komposteur bridge
examples/komposition-examples/     # Templates
```

## ‚ö†Ô∏è **LOCAL CI BUILD SYSTEM** ‚ö†Ô∏è

### **CI Build Strategy** ‚úÖ **IMPLEMENTATION REQUIRED**

**Local CI Command:**
```bash
# Run complete CI validation before push
uv run python test_basic_ci.py && echo "‚úÖ CI PASSED - Ready to push"
```

**CI Components:**
1. **Basic Validation**: `test_basic_ci.py` - Code structure and documentation
2. **Build Detective Analysis**: Automated quality assessment post-commit
3. **Integration Testing**: Video format strategy validation

**Pre-Push Workflow:**
```bash
# 1. Run local CI tests
uv run python test_basic_ci.py

# 2. If tests pass, commit and push
git add . && git commit -m "Feature implementation"
git push origin feature-branch

# 3. BD automatically analyzes pushed changes
# (BD report available within minutes of push)
```

**Future Git Hook Integration:**
- **NOT ENABLED YET** - User must observe CI build performance first
- **When ready**: Add `test_basic_ci.py` to pre-push hook
- **Estimated time**: ~5-10 seconds for basic validation

**BD Integration:**
- BD automatically analyzes all pushed branches
- BD reports available via Build Detective subagent
- Use BD confidence scores to validate positive changes

## ‚ö†Ô∏è **CRITICAL: LLM OVER-ENGINEERING ANTI-PATTERNS** ‚ö†Ô∏è

### **PR 22 Case Study: The Danger of Symptom-Fixing** üö®

**LESSON LEARNED**: LLMs can create "whack-a-mole" patterns that make simple problems exponentially more complex.

#### **The Symptom-Fixing Cycle**
```
Real Issue: Missing try/except around `import docker` (3 lines)
‚Üì
LLM Fix 1: "Missing temp/ files" ‚Üí Dockerfile.ci changes
‚Üì  
LLM Fix 2: "Module import errors" ‚Üí src/ directory restructuring
‚Üì
LLM Fix 3: "Complex workflow issues" ‚Üí 301-line workflow replacement
‚Üì
LLM Fix 4: "Merge conflicts" ‚Üí Complex resolution strategy
‚Üì
LLM Fix 5: "Dependency issues" ‚Üí pyproject.toml restructuring
‚Üì
Real Fix: try/except ImportError (3 lines) ‚úÖ
```

#### **WARNING SIGNS of LLM Over-Engineering** 
- ‚ùå **Multiple successive "fixes"** for the same core issue
- ‚ùå **Each fix touches 5+ files** or adds complex configuration
- ‚ùå **Local tests pass but CI fails repeatedly** 
- ‚ùå **"Simple" fixes require architectural changes**
- ‚ùå **Solution complexity >> problem complexity**

#### **MANDATORY PROTOCOL: Root Cause Analysis First**
```bash
# BEFORE implementing any "fix":
1. Ask: "Is this treating a symptom or the root cause?"
2. Compare CI environment vs local environment EXACTLY
3. Check: Does main branch have this issue?
4. Look for the SIMPLEST explanation first
5. If fix requires >10 lines, PAUSE and reconsider approach
```

#### **Success Pattern: CI Environment Comparison**
- **What worked**: Compare CI scripts between branches
- **What worked**: Check exact import failures in CI logs  
- **What worked**: Make the failing import optional (try/except)
- **What failed**: Complex dependency management, workflow rewriting, Docker restructuring

#### **Key Learning**: 
**"90% of the time CI failures are due to LLM code changes"** - The user was absolutely correct. LLMs tend to add complexity instead of finding the simple root cause.

#### **Validation Protocol**:
```python
# Always make optional imports when dealing with CI issues:
try:
    import optional_heavy_dependency
except ImportError:
    optional_heavy_dependency = None  # Graceful degradation
```

#### **Documentation Requirement**:
For any multi-fix PR, document:
- What was the original simple problem?
- How many "fixes" were attempted?  
- What was the actual root cause?
- Could this have been solved in 1-3 lines initially?

### **Build Detective Evolution**: From Problem to Solution
- **Problem**: CI failures recurring despite multiple fixes
- **Root Cause Discovery**: Compare main branch vs feature branch CI
- **Simple Solution**: Optional import pattern
- **Learning**: Always check environment differences first, not configuration complexity

This case study demonstrates that **simpler is usually correct** when dealing with import/dependency issues in CI environments.

### **CRITICAL DISTINCTION: Good vs Bad Multi-Fix Patterns** üéØ

#### **‚úÖ GOOD Pattern: Sequential Problem Discovery (Gemini Example)**
*From gemini-ffmpeg changes_summary_20250824.md - demonstrates proper systematic debugging*

```
Problem 1: Method call mismatch ‚Üí Fix: Load file + use correct async method ‚Üí New error
Problem 2: JSON structure mismatch ‚Üí Fix: Add sources list + sourceRef pattern ‚Üí New error  
Problem 3: File path restrictions ‚Üí Fix: Update FileManager config + hardcoded paths ‚Üí Success
```

**Why This Was CORRECT Multi-Fix Approach**:
- ‚úÖ **Each fix revealed NEW, SPECIFIC error message** (progress indicators)
- ‚úÖ **Targeted changes** addressing actual underlying issues  
- ‚úÖ **No architectural rewrites** - just corrected specific mismatches
- ‚úÖ **Sequential problem discovery** - fix A reveals problem B

#### **‚ùå BAD Pattern: Same Problem Escalating Complexity (PR 22)**
```
Same CI Failure ‚Üí Complex Dockerfile fix ‚Üí Still failing ‚Üí Workflow rewrite ‚Üí Still failing ‚Üí etc.
```

**Why This Was WRONG Multi-Fix Approach**:
- ‚ùå **Same error recurring** despite multiple "fixes"
- ‚ùå **Each fix more complex/architectural** than the last
- ‚ùå **Solution >> Problem Complexity** (50+ line fixes for import issue)
- ‚ùå **Symptom-fixing cycle** without root cause identification

### **Refined Over-Engineering Detection** üîç

#### **REAL Warning Signs** (Replace crude metrics)
- ‚ùå **Same Error Persisting** after 2+ attempts with different approaches
- ‚ùå **Architectural Drift** - "simple" fix requiring tech stack changes  
- ‚ùå **Solution >> Problem Complexity** - 50-line fix for 3-line problem
- ‚ùå **Recursive Complexity** - each fix creates more problems than it solves

#### **NOT Warning Signs** (False positives to avoid)
- ‚úÖ **Multiple files changed** when each change addresses specific, different issues
- ‚úÖ **Sequential debugging** that progresses through distinct problems
- ‚úÖ **Multiple fix attempts** when each attempt targets a newly discovered issue

#### **Key Insight: Error Message Patterns**
```bash
# GOOD Sequential Pattern:
Error 1: "Method 'process_komposition_file' not found" ‚Üí Fix method call
Error 2: "No source found for segment: None" ‚Üí Fix JSON structure  
Error 3: "File path not allowed: /tmp/music/source/..." ‚Üí Fix path handling

# BAD Recurring Pattern:  
Error: "ModuleNotFoundError: No module named 'docker'" ‚Üí Fix 1
Error: "ModuleNotFoundError: No module named 'docker'" ‚Üí Fix 2
Error: "ModuleNotFoundError: No module named 'docker'" ‚Üí Fix 3
```

**CRITICAL LEARNING**: **Quality of analysis** matters more than **quantity of changes**. Systematic debugging applied to multiple sequential issues is exactly what we want - not over-engineering.

## ‚ö†Ô∏è **COMPREHENSIVE AI RESTRAINT PROTOCOLS** ‚ö†Ô∏è

### **Research-Based Constraint Framework** üìö
*Based on analysis of Anthropic Engineering, The Ground Truth, Spec-Driven Development, and Claude Code documentation*

#### **The "3-Line Rule"** üéØ
**Principle**: If root cause is likely <10 lines of code, spend MORE time analyzing than implementing.

**Protocol**:
1. **Analysis Phase**: Use Build Detective, compare branches, identify patterns  
2. **Minimal Fix Hypothesis**: Propose simplest possible solution FIRST
3. **Escalation Threshold**: Only expand scope with explicit user approval

#### **The "Stable Point Protocol"** üîó  
**Principle**: Always work from known-good state ‚Üí known-good state.

**Steps**:
1. **Verify Starting Point**: Ensure current state compiles/tests pass
2. **Single Change**: Make one logical change only
3. **Verification**: Confirm new state is stable (BD local CI)
4. **Commit**: Version the change before next modification

#### **Pre-Implementation Constraint Gates** üö™

**MANDATORY CHECKPOINTS**:
- [ ] **Planning Phase**: Force AI to plan before ANY code changes
- [ ] **Root Cause Analysis**: Compare branches, identify simplest solution
- [ ] **Scope Definition**: Define maximum files/lines to change
- [ ] **YOLO Mode Approval**: Require explicit permission for >3 file changes

**AUTOMATIC TRIGGERS** (Refined based on Gemini vs PR 22 analysis):
- **Same Error Persisting** ‚Üí After 2+ attempts, switch to BD/environment comparison
- **Architectural Drift** ‚Üí "Simple" fix requiring tech stack changes ‚Üí STOP and ask  
- **Recursive Complexity** ‚Üí Each fix creates more problems ‚Üí STOP and reassess
- **Solution >> Problem** ‚Üí 50+ line fix for single error message ‚Üí Justify or simplify

#### **The Domain Expert Consultation Protocol** ü§ù
**Principle**: Senior developers know the codebase; ask instead of assuming.

**Implementation**:
```bash
# Before ANY architectural change:
1. "What's the typical pattern for X in this codebase?" 
2. "Has this issue occurred before?"
3. "Do you prefer approach A or B for this type of change?"
4. "Will this change affect any other systems?"
```

#### **Course Correction Protocols** üõë

**Interrupt Capabilities**:
- **ESC Key Protocol**: Stop mid-process when complexity escalates
- **Token Budget Awareness**: Reassess after significant token usage  
- **Build Detective First**: Use BD tools before LLM analysis for build issues
- **Branch Comparison**: When debugging, compare working vs broken branches FIRST

#### **Mature Codebase Sensitivity Rules** üèõÔ∏è

**GOLDEN RULES**:
1. **Incremental Over Comprehensive**: Small testable changes vs rewrites
2. **Backward Compatibility**: Maintain existing behavior patterns
3. **Domain Language**: Use project-specific terminology consistently  
4. **Change Impact Assessment**: Estimate affected systems before starting

**VIOLATION CONSEQUENCES**:
- **First Violation**: Warning and course correction
- **Second Violation**: Session reset with planning restart
- **Pattern Violation**: Immediate escalation to user oversight

### **Success Metrics & Monitoring** üìä

**Target Metrics**:
- **Progressive Error Resolution**: Each fix produces new, more specific error (not same recurring error)
- **First-Fix Success**: >80% problems solved without architectural changes
- **BD Usage Rate**: >90% build issues use BD before LLM analysis  
- **User Satisfaction**: Reduced overwhelming change review burden

**Warning Indicators** (Refined):
- **Error Message Stagnation**: Same error after multiple different approaches
- **Complexity Escalation**: Each successive fix more architectural than previous
- **Recursive Problem Creation**: Fixes generate more issues than they solve
- **Token Burn Without Progress**: High usage without advancing through distinct problems

### **Implementation Roadmap** üó∫Ô∏è

**Phase 1 (Immediate)**:
- [ ] Add explicit approval gates for >3 file changes
- [ ] Implement "pause and ask" checkpoints during complex tasks
- [ ] Create change impact estimator before modifications

**Phase 2 (Short-term)**:  
- [ ] Build Detective first protocol for all CI failures
- [ ] Subagent specialization with domain-specific constraints
- [ ] Automated scope detection based on task analysis

**Phase 3 (Long-term)**:
- [ ] Change complexity scoring with escalation thresholds
- [ ] Historical pattern learning from over-engineering incidents
- [ ] Continuous improvement feedback loops

**CRITICAL SUCCESS FACTOR**: Balancing AI capability with mature codebase sensitivity through explicit approval gates and domain expert consultation protocols.

## LLM Issue Reporting and Improvement Tracking

### Komposteur and Video Renderer Improvement Guidelines
- If LLM identifies needed changes/fixes in sub-process/library Komposteur, video-renderer etc, write a comprehensive report for Claude agents responsible for these, detailing:
  - Specific issues discovered
  - Current state of the component
  - Desired future state and recommended improvements
  - Potential impact on overall system performance
- this project is a learning process. Each run is to validate our assumptions, find flaws and improvements. MCP server is for exploratory, Komposteur is for make it production ready with lessons learned